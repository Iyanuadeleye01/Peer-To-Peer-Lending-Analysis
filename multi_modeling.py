# -*- coding: utf-8 -*-
"""multi_modeling.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BPMXYwNBI5s0_FMnaNIiVONMVHCHtK1p
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
# %matplotlib inline
import seaborn as sns
from sklearn.decomposition import PCA
from sklearn.feature_selection import mutual_info_regression, SelectPercentile
from sklearn.feature_selection import RFE
from sklearn.ensemble import RandomForestRegressor

pd.pandas.set_option('display.max_columns', None)

from google.colab import files
uploaded = files.upload()

dataset = pd.read_csv("prosperLoanData.csv")

dataset.shape

dataset.head()

dataset['LoanStatus']

# Checking for duplicates

dataset.duplicated().any()

dataset.info()

# Dataset statistics

dataset.describe()

"""Missing values"""

dataset.isnull().sum()

# Displaying all the features that has missing values.

null_columns = dataset.columns[dataset.isnull().any()]
print(null_columns)

print(len(null_columns))

"""43 columns contains null values."""

# Percentage of missing values present.

features_with_na = [features for features in dataset.columns if dataset[features].isnull().sum()>1]

for feature in features_with_na:
    print(feature, np.round(dataset[feature].isnull().mean(), 4),  ' % missing values')

"""Removing the features that has more than 80% null values."""

# Calculate the percentage of missing values for each column
null_percentage = dataset.isnull().mean() * 100

# Filter columns that have less than 80% missing values
data = dataset.loc[:, null_percentage < 80]

data.head()

data.shape

"""Displaying the percentage of missing values."""

features_with_na = [features for features in data.columns if data[features].isnull().sum()>1]

for feature in features_with_na:
    print(feature, np.round(data[feature].isnull().mean(), 4),  ' % missing values')

"""# Handling missing values in categorical variables.

Replacing all the missing values in the categorical features with new label called 'Missing'.
"""

# Get all the categorical features from the missing features (features_with_na).

cat_features_nan = [feature for feature in features_with_na if data[feature].dtypes=='object']

print(cat_features_nan)

## Replace missing values with a new label
def replace_cat_feature(data, cat_features_nan):
    data_new = data.copy()
    data_new[cat_features_nan] = data_new[cat_features_nan].fillna('Missing')
    return data_new

data = replace_cat_feature(data, cat_features_nan)

data[cat_features_nan].isnull().sum()

"""# Handling missing values in Numerical features."""

# Get all the numerical features from the missing features (features_with_na).

num_features_nan = [feature for feature in features_with_na if data[feature].dtypes != 'object']

print(num_features_nan)

for feature in num_features_nan:
    print(feature, np.round(data[feature].isnull().mean(), 4),  ' % missing values')

"""Finding out if the above numerical features containing missing values has outliers."""

for feature in num_features_nan:
    data.boxplot(column=feature)
    plt.ylabel(feature)
    plt.title(feature)
    plt.show()

"""# Almost all the numerical features containing null values has outliers. So, replacing all null values with median."""

# Replacing missing values of the numerical features with median.

for feature in num_features_nan:
    ## Replacing missing values by using median since there are outliers
    median_value = data[feature].median()
    data[feature].fillna(median_value, inplace=True)

data[num_features_nan].isnull().sum()

data.isnull().sum()

data.shape

"""All the null values has been removed."""

data.head()

"""# LoanStatus is the dependent feature and it has multiple categories such as Cancelled, Chargedoff, Completed, Current, Defaulted, FinalPaymentInProgress and PastDue.

Distribution of LoanStatus variable
"""

# Pie-chart

target = data['LoanStatus'].value_counts()

fig1, ax1 = plt.subplots()

ax1.pie(target, labels=target.index, autopct='%1.1f%%', shadow=None)
ax1.axis('equal')
plt.title("Distribution of LoanStatus", fontsize=14)
plt.show()

# Bar plot

# Get the count of unique values in the 'Category' column
target = data['LoanStatus'].value_counts()

# Create the bar plot
plt.bar(target.index, target.values)

# Adding title and labels
plt.title('Count of Each Category')
plt.xlabel('Loan Status')
plt.ylabel('Count')

plt.xticks(rotation=90)

# Show the plot
plt.show()

# Counts of each category in LoanStatus.

loan_status_counts = data['LoanStatus'].value_counts()

loan_status_df = pd.DataFrame({
    'Loan Status': loan_status_counts.index,
    'Counts': loan_status_counts.values
})

loan_status_df

"""Classifying different categories of loan status to binary as 'Defaulted': 1 and Non-Defaulted: 0."""

# def classify_loan(row):
#     # If LoanStatus is already "Defaulted" or "Chargedoff", mark as Defaulted
#     if row['LoanStatus'] in ['Defaulted', 'Chargedoff']:
#         return 1

#     # If LoanCurrentDaysDelinquent exceeds 90 days and the loan is closed (has a ClosedDate), mark as Defaulted
#     elif row['LoanCurrentDaysDelinquent'] > 180 and pd.notnull(row['ClosedDate']):
#         return 1

#     # Otherwise, consider the loan Not Defaulted
#     else:
#         return 0

# # Apply the classification logic

# data['IsDefaulted'] = data.apply(classify_loan, axis=1)

# data['IsDefaulted'].value_counts()

# Now remove LoanStatus column.

# data = data.drop(['LoanStatus'], axis=1)

data.shape

data.head()

"""The features "ListingKey", "ListingNumber", "LoanKey", "LoanNumber" and "MemberKey" are all unique keys which doesn't provide significant contribution to the analysis. So, it must be removed."""

data = data.drop(['ListingKey', 'ListingNumber', 'LoanKey', 'LoanNumber', 'MemberKey'], axis=1)

data.head()

# The feature "ProsperScore" should have values between 1-10. But, it also has a value of 11. So, replacing it to 10.

ProsperScore_counts = data['ProsperScore'].value_counts()

ProsperScore_df = pd.DataFrame({
    'Prosper Score': ProsperScore_counts.index,
    'Counts': ProsperScore_counts.values
})

ProsperScore_df

# ProsperScore feature has values between 1-10. But, we have the value '11'. So, replacing it by 10 as it indicates the lowest risk score.

data['ProsperScore'] = data['ProsperScore'].replace(11, 10)

data['ProsperScore'].unique()

data.head(20)

"""Almost all numerical features contains outliers and the numerical features are also skewed.

Applying "Yeo-Johnson Transformation" to the numeric columns as it can handle both outliers and positive/negative skewness.
"""

numeric_cols = data.select_dtypes(include=['int64', 'float64']).columns
numeric_cols

# Remove the 'IsDefaulted' column as it is the dependent feature.

numeric_cols = pd.Index(numeric_cols[:-1])

# from sklearn.preprocessing import PowerTransformer
# pt = PowerTransformer(method='yeo-johnson')

# for feature in numeric_cols:
#     data[feature] = pt.fit_transform(data[[feature]])

data.dtypes.unique()

"""# Dropping highly correlated features to handle "Multicollinearity"."""

# Numerical features (Both continuous and discrete)

numerical_features = [feature for feature in data.columns if data[feature].dtypes != 'O' and data[feature].dtype != 'bool']
numerical_features

# Create a correlation matrix

correlation_matrix = data[numerical_features].corr()

# Create a heatmap to visualize the correlation matrix
plt.figure(figsize=(28, 25))
sns.heatmap(correlation_matrix, annot=True, fmt=".2f", cmap='coolwarm', square=True) # cbar_kws={"shrink": .8}
plt.title('Correlation Matrix of Numeric Features')
plt.show()

threshold = 0.75

# Find pairs of features that are highly correlated
high_corr_pairs = []

# Iterate over the matrix and identify pairs with high correlation
for i in range(len(correlation_matrix.columns)):
    for j in range(i):
        if abs(correlation_matrix.iloc[i, j]) > threshold:  # Check for high correlation
            col1 = correlation_matrix.columns[i]
            col2 = correlation_matrix.columns[j]
            corr_value = correlation_matrix.iloc[i, j]
            high_corr_pairs.append((col1, col2, corr_value))

# Convert to DataFrame for better visualization
high_corr_df = pd.DataFrame(high_corr_pairs, columns=["Feature 1", "Feature 2", "Correlation"])

# Display the list of highly correlated features
print("Highly correlated feature pairs:")
print(high_corr_df)

# Removing the "IsDefault" feature from numerical_features list as it is the dependent feature.

# numerical_features.remove('IsDefaulted')

numerical_features

# Performing correlation.

correlation_matrix = data[numerical_features].corr()

# Identify the features that are highly correlated.

num_df = data[numerical_features]

# Create an upper triangle matrix to avoid duplicate pairs
upper_tri = correlation_matrix.where(np.triu(np.ones(correlation_matrix.shape), k=1).astype(bool))

# Set a threshold value for dropping correlated features
threshold = 0.7

# Find the columns that have correlations above the threshold
to_drop = [column for column in upper_tri.columns if any(upper_tri[column] > threshold)]

print(to_drop)

# Drop the correlated features
reduced_df = num_df.drop(columns=to_drop)

# Show the remaining features
print("Dropped features:", to_drop)
print("Remaining features after dropping correlated ones:")
print(reduced_df.head())

data.head()

corre = data['LP_CustomerPrincipalPayments'].corr(data['LP_CustomerPayments'])

corre

data.shape

# Dropping the highly correlated featured

# data.drop(['BorrowerAPR', 'LenderYield', 'EstimatedEffectiveYield', 'EstimatedLoss', 'EstimatedReturn', 'ProsperScore', 'CreditScoreRangeUpper', 'OpenCreditLines', 'OpenRevolvingAccounts', 'OpenRevolvingMonthlyPayment', 'AmountDelinquent', 'RevolvingCreditBalance', 'TotalTrades', 'MonthlyLoanPayment', 'LP_InterestandFees', 'InvestmentFromFriendsAmount'], axis = 1, inplace=True)

# Numerical features (Both continuous and discrete)

numerical_features = [feature for feature in data.columns if data[feature].dtypes != 'O' and data[feature].dtype != 'bool']
numerical_features

"""Handling dateTime features."""

# Categorical features

cat_feat = [feature for feature in data.columns if feature not in numerical_features]
cat_feat

"""Getting all dateTime related features"""

date_features = data[['ListingCreationDate', 'ClosedDate', 'DateCreditPulled', 'FirstRecordedCreditLine', 'LoanOriginationDate', 'LoanOriginationQuarter']]

date_features

"""Date features with null values

1.  ListingCreationDate
2.  DateCreditPulled

Date features which contains 'missing' string

1.  ClosedDate
2.  FirstRecordedCreditLine
"""

# Converting 'ListingCreationDate' column to dateTime type

# Create a backup of the original string column
data['Original_ListingCreationDate'] = data['ListingCreationDate'].astype(str)

# Convert to datetime with errors='coerce' (NaT for invalid values)
data['ListingCreationDate'] = pd.to_datetime(data['ListingCreationDate'], errors='coerce')

# Replace NaT values with the original string values
data['ListingCreationDate'] = data['ListingCreationDate'].fillna(data['Original_ListingCreationDate'])

data.drop(['Original_ListingCreationDate'], axis = 1, inplace = True)

data['ListingCreationDate'].head()

# Converting 'DateCreditPulled' column to dateTime type

# Create a backup of the original string column
data['Original_DateCreditPulled'] = data['DateCreditPulled'].astype(str)

# Convert to datetime with errors='coerce' (NaT for invalid values)
data['DateCreditPulled'] = pd.to_datetime(data['DateCreditPulled'], errors='coerce')

# Replace NaT values with the original string values
data['DateCreditPulled'] = data['DateCreditPulled'].fillna(data['Original_DateCreditPulled'])

data.drop(['Original_DateCreditPulled'], axis = 1, inplace = True)

data['DateCreditPulled'].head()

data['LoanOriginationDate'] = pd.to_datetime(data['LoanOriginationDate'])

"""Checking for missing values in Date time columns"""

is_present = data['ListingCreationDate'].isin(["Missing"]).any()
print(is_present)

is_present = data['DateCreditPulled'].isin(["Missing"]).any()
print(is_present)

is_present = data['ClosedDate'].isin(["Missing"]).any()
print(is_present)

is_present = data['LoanOriginationDate'].isin(["Missing"]).any()
print(is_present)

is_present = data['FirstRecordedCreditLine'].isin(["Missing"]).any()
print(is_present)

"""# Loan Tenure"""

date_cols = ["ClosedDate", "LoanOriginationDate", "ListingCreationDate"]

data.dropna()

data.shape

for col in date_cols:
    data[col] = pd.to_datetime(data[col], errors='coerce')

data.shape

print(data.shape)

data[date_cols].head()

"""**LoanTenure = (ð‘€ð‘Žð‘¡ð‘¢ð‘Ÿð‘–ð‘¡ð‘¦ð·ð‘Žð‘¡_ð‘‚ð‘Ÿð‘–ð‘”ð‘–ð‘›ð‘Žð‘™ð‘¦ð‘’ð‘Žð‘Ÿ âˆ’ ð¿ð‘œð‘Žð‘›ð·ð‘Žð‘¡ð‘’ð‘¦ð‘’ð‘Žð‘Ÿ) ð‘¥ 12 âˆ’
(ð‘€ð‘Žð‘¡ð‘¢ð‘Ÿð‘–ð‘¡ð‘¦ð·ð‘Žð‘¡ð‘’_ð‘‚ð‘Ÿð‘–ð‘”ð‘–ð‘›ð‘Žð‘™ð‘šð‘œð‘›ð‘¡â„Ž âˆ’ ð¿ð‘œð‘Žð‘›ð·ð‘Žð‘¡ð‘’ð‘šð‘œð‘›ð‘¡â„Ž)**
"""

data["LoanTenure"] = ((data["ClosedDate"].dt.year - data["LoanOriginationDate"].dt.year) * 12) - (data["ClosedDate"].dt.month - data["LoanOriginationDate"].dt.month)

print(data["LoanTenure"].shape)

data.shape

data["LoanTenure"].describe()

print(data["Term"].describe())

print(data["LoanTenure"].describe())

data.drop(date_cols, axis=1, inplace=True)

data.drop("LoanTenure", axis=1, inplace=True)

data.rename(columns={"Term" : "LoanTenure"}, inplace=True)

data.shape

"""# Equated Monthly Installments (EMI)

-	Tenure ---> **LoanTenure**
-	Principle repayment ---> **LP_CustomerPrinciplePayments**
-	Interest ---> **BorrowerRate**
"""

emi_cols = ["LP_CustomerPayments", "LP_CustomerPrincipalPayments"]

data[emi_cols].head()

"""**For each row in the dataset:**
1. Calculate result_1 = P * r * ã€–(1+r)ã€—^n
2. Calculate result_2 = ã€–(1+r)ã€—^n â€“ 1
3. Calculate EMI = result_1 / result_2
"""

def cal_EMI(P, r, n):
  P = P.values
  r = r.values
  n = n.values
  #print(P.shape[0])
  result_1 = np.empty(0)
  result_2 = np.empty(0)
  result = np.empty(0)
  for i in range(P.shape[0]):
    #print(P[i])
    #print(r[i])
    #print(n[i])
    # EMI = P Ã— r Ã— (1 + r) ^ n / ((1 + r) ^ n â€“ 1)
    #print(P[i] * (1 + r[i]))
    result_1 = np.append(result_1, P[i] * r[i] * np.power((1 + r[i]),n[i]))
    result_2 = np.append(result_2, np.power((1 + r[i]),n[i]) - 1)
    result = np.append(result, (result_1[i] / result_2[i]))

  return result

data["BorrowerRate"].describe()

data["EMI"] = cal_EMI(data["LP_CustomerPrincipalPayments"], data["BorrowerRate"], data["LoanTenure"])

data["EMI"].describe()

"""# Eligible Loan Amount (ELA)

**Components of ELA:**
-	A: â€œAppliedAmountâ€ ---> **LoanOriginalAmount**
-	R: â€œInterestâ€ ---> **BorrowerRate**
-	N: â€œLoanTenureâ€ ---> **LoanTenure**
-	I: â€œIncomeTotalâ€  ---> **StatedMonthlyIncome**
"""

ela_cols = ['DebtToIncomeRatio', 'IncomeRange', 'IncomeVerifiable', 'StatedMonthlyIncome']

data[ela_cols].head()

df_new = data[data['IncomeVerifiable'] == True]

print(df_new.shape)

print(data.shape[0] - df_new.shape[0])

"""**Calculation Procedure:**
**For each row in the dataset:**
1.	Calculate: Total Payment Due = (A + (A*r)) * n
2.	Calculate: Max allowable amount = I * 12 * 30%
3.	If ( Total Payment Due <= Max allowable amount)
            
            Then ELA = AppliedAmount
            Else ELA = Max allowable amount
"""

def eligible_loan_amnt(df):
  df['Ava_Inc'] = (df['StatedMonthlyIncome'] * 12 * 0.3) * df['LoanTenure']
  df['Total_Loan_Amnt'] = np.round(df['LoanOriginalAmount'] + (df['LoanOriginalAmount'] * df['BorrowerRate']) *df['LoanTenure'])

  ELA = np.empty(0)

  for i in range(len(df['Ava_Inc'].values)):
    if df['Ava_Inc'].iloc[i] <= 0:
      ELA = np.append(ELA, 0)
    elif df['Total_Loan_Amnt'].iloc[i] <= df['Ava_Inc'].iloc[i]:
      ELA = np.append(ELA, df['Total_Loan_Amnt'].iloc[i])
    else:
      ELA = np.append(ELA, df['Ava_Inc'].iloc[i])

  df.drop(["Ava_Inc", "Total_Loan_Amnt"], axis=1, inplace=True)

  return ELA

data['ELA'] = eligible_loan_amnt(data)

data['ELA'].describe()

data['ELA']

data.shape

"""# Preferred Return on Investment (PROI)"""

# The function displays a graph.
def plot_by_woe(df_WoE, rotation_of_x_axis_labels = 0):
    x = np.array(df_WoE.iloc[:, 0].apply(str))
    # Turns the values of the column with index 0 to strings, makes an array from these strings, and passes it to variable x.
    y = df_WoE['WoE']
    # Selects a column with label 'WoE' and passes it to variable y.
    plt.figure(figsize=(18, 6))
    # Sets the graph size to width 18 x height 6.
    plt.plot(x, y, marker = 'o', linestyle = '--', color = 'k')
    # Plots the datapoints with coordiantes variable x on the x-axis and variable y on the y-axis.
    # Sets the marker for each datapoint to a circle, the style line between the points to dashed, and the color to black.
    plt.xlabel(df_WoE.columns[0])
    # Names the x-axis with the name of the column with index 0.
    plt.ylabel('Weight of Evidence')
    # Names the y-axis 'Weight of Evidence'.
    plt.title(str('Weight of Evidence by ' + df_WoE.columns[0]))
    # Names the grapth 'Weight of Evidence by ' the name of the column with index 0.
    plt.xticks(rotation = rotation_of_x_axis_labels)
    # Rotates the labels of the x-axis a predefined number of degrees.

"""## Simple Data Preprocessing"""

data.select_dtypes(exclude=[int, float]).columns

from sklearn.preprocessing import LabelEncoder

encoder = LabelEncoder()

cat_cols = data.select_dtypes(include=["object"]).columns

for col in cat_cols:
    data[col] = encoder.fit_transform(data[col])

cat_cols

bool_cols = ['IsBorrowerHomeowner', 'CurrentlyInGroup', 'IncomeVerifiable']

for col in bool_cols:
    data[col] = data[col].astype(int)

data.isnull().sum().sum()

data.dropna(inplace=True)

data.isnull().sum().sum()

data.select_dtypes(exclude=[int, float]).columns

data.dtypes

"""## Feature Importance Analysis"""

data.head()

data = data.drop(["DateCreditPulled"], axis=1)

data.shape

X = data.drop(["LoanStatus"], axis=1)

y = data["LoanStatus"]

print(data.shape)
print(y.shape)

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()

X_scale = scaler.fit_transform(X)

from sklearn.model_selection import train_test_split

#Train Test for Classification
X_class_train, X_class_test, y_class_train, y_class_test = train_test_split(X_scale, y, test_size=0.25, random_state=0)

from sklearn.ensemble import RandomForestRegressor

rid_reg = RandomForestRegressor()

rid_reg.fit(X_class_train, y_class_train)

y_pred_base = rid_reg.predict(X_class_test)

from sklearn.metrics import r2_score, mean_absolute_percentage_error, mean_squared_error

# model evaluation
print('Ridge Regression - Base Model:')
print('mean_squared_error : ', mean_squared_error(y_class_test, y_pred_base))
print('mean_squared_percentage_error : ', mean_absolute_percentage_error(y_class_test, y_pred_base))
print('R2_score : ', r2_score(y_class_test, y_pred_base))

result = pd.DataFrame(X.columns.values, columns=['feature_name'])
result['importance'] = rid_reg.feature_importances_
result.sort_values(by='importance', ascending=False, inplace=True)
result.head(10)

important_cols = result.head(20)['feature_name'].tolist()

"""## Analysis of Numerical Attributes"""

# WoE function for ordered discrete and continuous variables
def woe_continuous(df, discrete_variabe_name, good_bad_variable_df):
    df = pd.concat([df[discrete_variabe_name], good_bad_variable_df], axis = 1)
    df = pd.concat([df.groupby(df.columns.values[0], as_index = False)[df.columns.values[1]].count(),
                    df.groupby(df.columns.values[0], as_index = False)[df.columns.values[1]].mean()], axis = 1)
    df = df.iloc[:, [0, 1, 3]]
    df.columns = [df.columns.values[0], 'n_obs', 'prop_good']
    df['prop_n_obs'] = df['n_obs'] / df['n_obs'].sum()
    df['n_good'] = df['prop_good'] * df['n_obs']
    df['n_bad'] = (1 - df['prop_good']) * df['n_obs']
    df['prop_n_good'] = df['n_good'] / df['n_good'].sum()
    df['prop_n_bad'] = df['n_bad'] / df['n_bad'].sum()
    df['WoE'] = np.log(df['prop_n_good'] / df['prop_n_bad'])
    #df = df.sort_values(['WoE'])
    #df = df.reset_index(drop = True)
    df['diff_prop_good'] = df['prop_good'].diff().abs()
    df['diff_WoE'] = df['WoE'].diff().abs()
    df['IV'] = (df['prop_n_good'] - df['prop_n_bad']) * df['WoE']
    df['IV'] = df['IV'].sum()
    return df
# Here we define a function similar to the one above, ...
# ... with one slight difference: we order the results by the values of a different column.
# The function takes 3 arguments: a dataframe, a string, and a dataframe. The function returns a dataframe as a result.

important_cols = important_cols[:5]
important_cols

target = data["LoanStatus"]
df_new = data[['LP_CustomerPrincipalPayments', 'ProsperRating (Alpha)', 'LoanOriginalAmount', 'LoanCurrentDaysDelinquent', 'MonthlyLoanPayment']]
df_new.shape

df_new.select_dtypes(include=[int, float]).columns

"""#### **LP_CustomerPrincipalPayments**"""

pd.options.display.max_rows = None

df_new['LP_CustomerPrincipalPayments_factor'] = pd.cut(df_new['LP_CustomerPrincipalPayments'], 50)

temp = woe_continuous(df_new, 'LP_CustomerPrincipalPayments_factor', target)

temp

pd.options.display.max_rows = None

temp = df_new[df_new['LP_CustomerPrincipalPayments'] < 2000]
temp['LP_CustomerPrincipalPayments_factor'] = pd.cut(temp['LP_CustomerPrincipalPayments'], 30)

temp = woe_continuous(temp, 'LP_CustomerPrincipalPayments_factor', target)

temp

plot_by_woe(temp, 90)

"""* More than 10500
* Between 2000 and 10500
* Between 1000 and 2000
* Less than 200
"""

# Create categorie
df_new['LP_CustomerPrincipalPayments_factor'] = np.where(df_new['LP_CustomerPrincipalPayments'] < 200, "<200",
                                                    np.where(df_new['LP_CustomerPrincipalPayments'] < 1000, "200_1k",
                                                    np.where(df_new['LP_CustomerPrincipalPayments'] < 2000, "1k_2k",
                                                    np.where(df_new['LP_CustomerPrincipalPayments'] < 10500, "2k_10.5k", ">10.5k"))))
df_new['LP_CustomerPrincipalPayments_factor'].value_counts()

"""#### **ProsperRating (Alpha)**"""

df_new['ProsperRating_factor'] = df_new['ProsperRating (Alpha)'].map(lambda x: str(x))
print(df_new['ProsperRating_factor'].dtype)
df_new['ProsperRating_factor'].value_counts()

"""#### **LoanOriginalAmount**"""

df_new['LoanOriginalAmount_factor'] = pd.cut(df_new['LoanOriginalAmount'], 50)

temp = woe_continuous(df_new, 'LoanOriginalAmount_factor', target)

temp

plot_by_woe(temp, 90)

temp = df_new[df_new['LoanOriginalAmount'] <= 9500]

temp['LoanOriginalAmount_factor'] = pd.cut(temp['LoanOriginalAmount'], 50)

temp = woe_continuous(temp, 'LoanOriginalAmount_factor', target)

temp

plot_by_woe(temp, 90)

"""* < 2000
* 2000 < x < 3000
* 3000 < x < 3500
* 3500 < x < 4000
* 4000 < x < 5000
* 5000 < x < 7500
* 7500 < x < 9500
* 9500 < x < 14500
* 14500 < x < 19500
* 19500 < x < 25000
"""

def convert(x):
    if x <= 2000:
        return "<2k"
    elif x <= 3000:
        return "2k_3k"
    elif x <= 3500:
        return "3k_3.5k"
    elif x <= 4000:
        return "3.5k_4K"
    elif x <=5000:
        return "4k_5k"
    elif x <= 7500:
        return "5k_7.5k"
    elif x <= 9500:
        return "7.5k_9.5k"
    elif x <= 14500:
        return "9.5k_14.5k"
    elif x <= 19500:
        return "14.5k_19.5k"
    elif x <= 25000:
        return "19.5k_24.5k"
    else:
        return "25.5k<"

df_new['LoanOriginalAmount_factor'] = data['LoanOriginalAmount'].map(lambda x : convert(x))
df_new['LoanOriginalAmount_factor'].value_counts()

"""#### **LoanCurrentDaysDelinquent**"""

temp = df_new[df_new['LoanCurrentDaysDelinquent'] < 50]

temp['LoanCurrentDaysDelinquent_factor'] = pd.cut(temp['LoanCurrentDaysDelinquent'], 50)

temp = woe_continuous(temp, 'LoanCurrentDaysDelinquent_factor', target)

temp

plot_by_woe(temp, 90)

def convert(x):
    if x == 0:
        return "=0"
    elif x < 50:
        return "<50"
    else:
        return "50<"

df_new['LoanCurrentDaysDelinquent_factor'] = df_new['LoanCurrentDaysDelinquent'].map(lambda x : convert(x))
df_new['LoanCurrentDaysDelinquent_factor'].value_counts()

"""#### **MonthlyLoanPayment**"""

df_new['MonthlyLoanPayment_factor'] = pd.cut(df_new['MonthlyLoanPayment'], 50)

temp = woe_continuous(df_new, 'MonthlyLoanPayment_factor', target)

temp

plot_by_woe(temp, 90)

temp = df_new[df_new['MonthlyLoanPayment'] > 90]

temp['MonthlyLoanPayment_factor'] = pd.cut(temp['MonthlyLoanPayment'], 50)

temp = woe_continuous(temp, 'MonthlyLoanPayment_factor', target)

temp

plot_by_woe(temp, 90)

def convert(x):
    if x <= 90:
        return "<90"
    elif x <= 225:
        return "90_225"
    elif x <= 360:
        return "225_360"
    elif x <= 600:
        return "360_600"
    elif x <= 750:
        return "600_750"
    else:
        return "750<"

df_new['MonthlyLoanPayment_factor'] = df_new['MonthlyLoanPayment'].map(lambda x : convert(x))
df_new['MonthlyLoanPayment_factor'].value_counts()

"""## Analysis of Categorical Attributes"""

# WoE function for discrete unordered variables
def woe_discrete(df, discrete_variabe_name, good_bad_variable_df):
    df = pd.concat([df[discrete_variabe_name], good_bad_variable_df], axis = 1)
    df = pd.concat([df.groupby(df.columns.values[0], as_index = False)[df.columns.values[1]].count(),
                    df.groupby(df.columns.values[0], as_index = False)[df.columns.values[1]].mean()], axis = 1)
    df = df.iloc[:, [0, 1, 3]]
    df.columns = [df.columns.values[0], 'n_obs', 'prop_good']
    df['prop_n_obs'] = df['n_obs'] / df['n_obs'].sum()
    df['n_good'] = df['prop_good'] * df['n_obs']
    df['n_bad'] = (1 - df['prop_good']) * df['n_obs']
    df['prop_n_good'] = df['n_good'] / df['n_good'].sum()
    df['prop_n_bad'] = df['n_bad'] / df['n_bad'].sum()
    df['WoE'] = np.log(df['prop_n_good'] / df['prop_n_bad'])
    df = df.sort_values(['WoE'])
    df = df.reset_index(drop = True)
    df['diff_prop_good'] = df['prop_good'].diff().abs()
    df['diff_WoE'] = df['WoE'].diff().abs()
    df['IV'] = (df['prop_n_good'] - df['prop_n_bad']) * df['WoE']
    df['IV'] = df['IV'].sum()
    return df
# Here we combine all of the operations above in a function.
# The function takes 3 arguments: a dataframe, a string, and a dataframe. The function returns a dataframe as a result.

df_new = df_new[['LP_CustomerPrincipalPayments_factor', 'ProsperRating_factor',
       'LoanOriginalAmount_factor', 'LoanCurrentDaysDelinquent_factor',
       'MonthlyLoanPayment_factor']]
df_new.columns

"""#### **LP_CustomerPrincipalPayments_factor**"""

temp = woe_discrete(df_new, 'LP_CustomerPrincipalPayments_factor', target)
temp.sort_values(by = 'LP_CustomerPrincipalPayments_factor', inplace=True)
temp

plot_by_woe(temp, 90)

"""#### **ProsperRating_factor**"""

temp = woe_discrete(df_new, 'ProsperRating_factor', target)
temp.sort_values(by = 'ProsperRating_factor', inplace=True)
temp

plot_by_woe(temp, 90)

"""#### **LoanOriginalAmount_factor**"""

temp = woe_discrete(df_new, 'LoanOriginalAmount_factor', target)
temp.sort_values(by = 'LoanOriginalAmount_factor', inplace=True)
temp

plot_by_woe(temp, 90)

"""#### **LoanCurrentDaysDelinquent_factor**"""

temp = woe_discrete(df_new, 'LoanCurrentDaysDelinquent_factor', target)
temp.sort_values(by = 'LoanCurrentDaysDelinquent_factor', inplace=True)
temp

plot_by_woe(temp, 90)

"""#### **MonthlyLoanPayment_factor**"""

temp = woe_discrete(df_new, 'MonthlyLoanPayment_factor', target)
temp.sort_values(by = 'MonthlyLoanPayment_factor', inplace=True)
temp

plot_by_woe(temp, 90)

"""# Algorithm (PreferredROI)"""

def PROI(df):
    # Calculate ROI
    df['InterestAmount'] = (df['LoanOriginalAmount']*(df['BorrowerRate']))
    df['TotalAmount'] = (df['InterestAmount'] + df['LoanOriginalAmount'])
    df['ROI'] = (df['InterestAmount'] / df['TotalAmount'])
    print(df['ROI'].describe())

    # Setting PROI
    df['PROI'] = df['ROI'].median()

    for i in range(df.shape[0]):
        # Check out LP_CustomerPrinciplePayment
        if df['LP_CustomerPrincipalPayments'].iloc[i] <= 1000:
            df['PROI'].iloc[i] = df['PROI'].iloc[i] + 0.05
        elif (df['LP_CustomerPrincipalPayments'].iloc[i] > 2000) & (df['LP_CustomerPrincipalPayments'].iloc[i] <= 10500):
            df['PROI'].iloc[i] = df['PROI'].iloc[i] - 0.05
        elif (df['LP_CustomerPrincipalPayments'].iloc[i] > 10500):
            df['PROI'].iloc[i] = df['PROI'].iloc[i] - 0.1

        # Check out ProsperRating
        if df['ProsperRating (Alpha)'].iloc[i] in [2, 3]:
            df['PROI'].iloc[i] = df['PROI'].iloc[i] + 0.05
        elif df['ProsperRating (Alpha)'].iloc[i] == 6:
            df['PROI'].iloc[i] = df['PROI'].iloc[i] - 0.05

        # Check out LoanOriginalAmount
        if df['LoanOriginalAmount'].iloc[i] <= 2000:
            df['PROI'].iloc[i] = df['PROI'].iloc[i] - 0.05
        elif (df['LoanOriginalAmount'].iloc[i] > 19500) & (df['LoanOriginalAmount'].iloc[i] <= 25500):
            df['PROI'].iloc[i] = df['PROI'].iloc[i] + 0.05
        elif df['LoanOriginalAmount'].iloc[i] > 25500:
            df['PROI'].iloc[i] = df['PROI'].iloc[i] + 0.1

        # Check out LoanCurrentDaysDelinquent
        if (df['LoanCurrentDaysDelinquent'].iloc[i] >= 50):
            df['PROI'].iloc[i] = df['PROI'].iloc[i] + 0.05

        # Check out MonthlyLoanPayment
        if (df['MonthlyLoanPayment'].iloc[i] <= 90):
            df['PROI'].iloc[i] = df['PROI'].iloc[i] - 0.05
        elif (df['MonthlyLoanPayment'].iloc[i] <= 750) & (df['MonthlyLoanPayment'].iloc[i] > 360):
            df['PROI'].iloc[i] = df['PROI'].iloc[i] + 0.05

    print(df['ROI'].describe())

    return df['PROI']

data.head()

data['ELA']

data['PROI'] = PROI(data)

data['PROI'].describe()

data.head(20)

"""## **FEATURE REDUCTION**"""

# Define X (features) and y (target)

X = data.drop(columns=['EMI', 'ELA', 'PROI'], axis = 1)
y = data[['EMI', 'ELA', 'PROI']]

data.shape

X.shape

y.shape

"""#### **Principal Component Analysis**"""

# Standardize the features

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Apply PCA

pca = PCA()
X_pca = pca.fit_transform(X_scaled)

# Display explained variance ratio

explained_variance = pca.explained_variance_ratio_
print("Explained Variance Ratio for each component:", explained_variance)

# Choose the number of components

cumulative_variance = explained_variance.cumsum()
n_components_95 = next(i for i, total in enumerate(cumulative_variance) if total >= 0.95) + 1
print(f"Number of components to retain 95% variance: {n_components_95}")

# Reduce to n_components_95 dimensions

pca = PCA(n_components=n_components_95)
X_reduced = pca.fit_transform(X_scaled)

print("Shape of X after PCA:", X_reduced.shape)

"""Using PCA, 40 features will be selected which explains a cumulative explained variance of 95%

#### **Random Forest Regressor**
"""

# Create a multi-output regressor

regressor = RandomForestRegressor(random_state=42)

# Use RFE for feature selection

rfe = RFE(estimator=regressor, n_features_to_select=30)  # Specify number of features to select

X_selected = rfe.fit_transform(X, y)

selected_features_indices = rfe.get_support(indices=True)

print(f"Selected Features (indices): {selected_features_indices}")

print("Shape of X after RFE:", X_selected.shape)

"""#### **Feature selection using Mutual Information (with Multi-output Aggregation)**"""

# Ensure y is a NumPy array with proper shape
if not isinstance(y, np.ndarray):
    y = y.to_numpy()

# Compute mutual information for each target and aggregate scores

mi_scores = np.zeros(X.shape[1])

# y.shape[1] is the number of target variables

for i in range(y.shape[1]):
  # Compute mutual information for the i-th target variable
  mi_scores += mutual_info_regression(X, y[:, i], random_state=42)

mi_scores /= y.shape[1]  # Average scores across targets

# Select top features

percentile = 20

n_top_features = int(len(mi_scores) * percentile / 100)

# Get the indices of the top features

top_feature_indices = np.argsort(mi_scores)[-n_top_features:]

top_feature_indices

# Check if X is a NumPy array or pandas DataFrame

if isinstance(X, np.ndarray):
    # If X is a NumPy array, use this indexing
    X_selected = X[:, top_feature_indices]
elif isinstance(X, pd.DataFrame):
    # If X is a pandas DataFrame, use .iloc for indexing
    X_selected = X.iloc[:, top_feature_indices]

print(f"Selected Features (indices): {top_feature_indices}")

print(f"Shape of X after feature selection: {X_selected.shape}")

"""13 features are selected using Mutual Information with Multi-output aggregation feature selection."""









